_target_: src.models.cc_flowmatching_module.ClassConditionedFlowMatchingLitModule

unet:
  _target_: src.models.components.u_net.UNetModel
  image_size : 512
  in_channels : 2
  model_channels: 64
  out_channels : 2
  num_res_blocks : 2
  attention_resolutions : [4]
  dropout : 0.0
  channel_mult : [1, 2, 2, 4]
  conv_resample : true
  dims : 2
  n_classes : null
  use_checkpoint : false
  use_fp16 : false
  num_heads : 1
  num_head_channels :  -1
  num_heads_upsample : -1
  use_scale_shift_norm : false
  resblock_updown : false
  use_new_attention_order : false

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0001
  weight_decay: 0.0
  
# scheduler:
#   _target_: src.models.components.scheduler.schedulers.WarmupCosineScheduler
#   _partial_: true
#   num_warmup_steps: 40
#   num_training_steps: ${data.batch_size}
#   num_cycles: 0.5

FM_param:
  n_classes: ${model.unet.n_classes}
  dropout_prob: 0.0
  guidance_strength: 1.0
  step_size: 0.1
  warm_up: 0
  encode: false
  plot_n_epoch: 5
  batch_size: ${data.batch_size}
  solver_lib: torchdiffeq
  solver: euler
  pretrained_dir: 'C:\Users\lmohle\Documents\2_Coding\data\Trained_Models\AutoEncoderKL\'
  reconstruct: 1
  save_reconstructs: false
  target_index: 1
  mode: rgb
  wh: 1
  plot: false
  plot_ids: [0,1,16,17]
  ood: false
  win_size: 3

scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  _partial_: true
  max_lr: ${model.optimizer.lr}
  total_steps: ${trainer.max_epochs}
  pct_start: ${eval:'${model.FM_param.warm_up} / ${model.scheduler.total_steps}'}
  anneal_strategy: cos
  cycle_momentum: false
  div_factor: ${eval:'${model.scheduler.max_lr}/1e-6'}
  final_div_factor: 1

compile: false

paths: 
  log_dir: ${paths.output_dir}