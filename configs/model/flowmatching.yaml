_target_: src.models.flowmatching_module.FlowMatchingLitModule

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.0002
  weight_decay: 0.0

FM_param:
  step_size: 0.1
  warm_up: 0
  latent: false
  plot_n_epoch: 5
  batch_size: ${data.batch_size}
  size: 32
  solver_lib: torchdiffeq
  solver: euler
  pretrained: 'C:\Users\lmohle\Documents\2_Coding\data\Trained_Models\AutoEncoderKL\'
  sigma_min: 1e-4
  reconstruct: 1
  target: 1
  mode: rgb
  wh: 1
  plot_ids: [0,1,16,17]
  ood: false

# scheduler:
#   _target_: torch.optim.lr_scheduler.OneCycleLR
#   _partial_: true
#   max_lr: ${model.optimizer.lr}
#   total_steps: ${trainer.max_epochs}
#   pct_start: ${eval:'${model.FM_param.warm_up} / ${model.scheduler.total_steps}'}
#   anneal_strategy: cos
#   cycle_momentum: false
#   div_factor: ${eval:'${model.scheduler.max_lr}/1e-6'}
#   final_div_factor: 1
scheduler:
  _target_: src.models.components.scheduler.schedulers.WarmupCosineScheduler
  _partial_: true
  num_warmup_steps: 500
  num_training_steps: 6400
  num_cycles: 0.5

unet:
  _target_: diffusers.models.UNet2DModel
  act_fn: silu
  add_attention: true
  attention_head_dim: 8
  attn_norm_num_groups: null
  center_input_sample: false
  class_embed_type: null
  block_out_channels:
    - 128
    - 128
    - 256
    - 256
    - 512
    - 512
  down_block_types:
    - DownBlock2D
    - DownBlock2D
    - DownBlock2D
    - DownBlock2D
    - AttnDownBlock2D
    - DownBlock2D
  up_block_types:
    - UpBlock2D
    - AttnUpBlock2D
    - UpBlock2D
    - UpBlock2D
    - UpBlock2D
    - UpBlock2D
  downsample_padding: 1
  downsample_type: conv
  dropout: 0.0
  flip_sin_to_cos: true
  freq_shift: 0
  in_channels: 3
  layers_per_block: 2
  mid_block_scale_factor: 1
  norm_eps: 1.0e-05
  norm_num_groups: 32
  num_class_embeds: null
  num_train_timesteps: null
  out_channels: 3
  resnet_time_scale_shift: default
  sample_size: 32
  time_embedding_type: positional
  upsample_type: conv

compile: false

paths: 
  log_dir: ${paths.output_dir}